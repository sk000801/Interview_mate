from tensorflow.keras.utils import img_to_array
import imutils
from keras.models import load_model
import numpy as np
import cv2
from gaze_tracking import GazeTracking
import math
from face_detector import get_face_detector, find_faces
from face_landmarks import get_landmark_model, detect_marks
from flask import Flask, render_template, url_for, request, jsonify, Response
import PyPDF2
import io
import openai
import xml.etree.ElementTree as ET
import time as Time

openai.api_key = "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"
isEscape = False
fault = []

app = Flask(__name__)

## ê³ ê°œ ê°ë„ ê°ì§€ë¥¼ ìœ„í•´ ê°€ì ¸ì˜´
def get_2d_points(img, rotation_vector, translation_vector, camera_matrix, val):
    point_3d = []
    dist_coeffs = np.zeros((4,1))
    rear_size = val[0]
    rear_depth = val[1]
    point_3d.append((-rear_size, -rear_size, rear_depth))
    point_3d.append((-rear_size, rear_size, rear_depth))
    point_3d.append((rear_size, rear_size, rear_depth))
    point_3d.append((rear_size, -rear_size, rear_depth))
    point_3d.append((-rear_size, -rear_size, rear_depth))
    
    front_size = val[2]
    front_depth = val[3]
    point_3d.append((-front_size, -front_size, front_depth))
    point_3d.append((-front_size, front_size, front_depth))
    point_3d.append((front_size, front_size, front_depth))
    point_3d.append((front_size, -front_size, front_depth))
    point_3d.append((-front_size, -front_size, front_depth))
    point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)
    
    # Map to 2d img points
    (point_2d, _) = cv2.projectPoints(point_3d,
                                      rotation_vector,
                                      translation_vector,
                                      camera_matrix,
                                      dist_coeffs)
    point_2d = np.int32(point_2d.reshape(-1, 2))
    return point_2d

def draw_annotation_box(img, rotation_vector, translation_vector, camera_matrix,
                        rear_size=300, rear_depth=0, front_size=500, front_depth=400,
                        color=(255, 255, 0), line_width=2):
    rear_size = 1
    rear_depth = 0
    front_size = img.shape[1]
    front_depth = front_size*2
    val = [rear_size, rear_depth, front_size, front_depth]
    point_2d = get_2d_points(img, rotation_vector, translation_vector, camera_matrix, val)
    # # Draw all the lines
    cv2.polylines(img, [point_2d], True, color, line_width, cv2.LINE_AA)
    cv2.line(img, tuple(point_2d[1]), tuple(
        point_2d[6]), color, line_width, cv2.LINE_AA)
    cv2.line(img, tuple(point_2d[2]), tuple(
        point_2d[7]), color, line_width, cv2.LINE_AA)
    cv2.line(img, tuple(point_2d[3]), tuple(
        point_2d[8]), color, line_width, cv2.LINE_AA)
    
    
def head_pose_points(img, rotation_vector, translation_vector, camera_matrix):
    rear_size = 1
    rear_depth = 0
    front_size = img.shape[1]
    front_depth = front_size*2
    val = [rear_size, rear_depth, front_size, front_depth]
    point_2d = get_2d_points(img, rotation_vector, translation_vector, camera_matrix, val)
    y = (point_2d[5] + point_2d[8])//2
    x = point_2d[2]
    
    return (x, y)

gaze = GazeTracking()
webcam = cv2.VideoCapture(0)

## ê°ì •ë¶„ì„ê³¼ ë™ê³µ ì›€ì§ì„ ì½”ë“œê°€ ìœ ì‚¬í•´ì„œ ì¼ë‹¨ ê°€ì ¸ì˜´
# parameters for loading data and images
detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'
emotion_model_path = 'models/_mini_XCEPTION.102-0.66.hdf5'
# hyper-parameters for bounding boxes shape
# loading models
face_detection = cv2.CascadeClassifier(detection_model_path)
emotion_classifier = load_model(emotion_model_path, compile=False)
face_model = get_face_detector()
landmark_model = get_landmark_model()
EMOTIONS = ["angry" ,"disgust","scared", "happy", "sad", "surprised",
 "neutral"]
emot=[0,0,0,0,0,0,0] # ê°ì •íšŸìˆ˜ ë³„ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸

def start():
    firstLeftEye = []
    firstRightEye = []
    frameCount = 0
    eyeErr = 0
    startTime = 0
    while True:
        if isEscape:
            break
        if(frameCount == 0):
            startTime = Time.time()

        # We get a new frame from the webcam
        _, frame = webcam.read()
        ##ê°ë„ì›€ì§ì„
        ret, img = webcam.read()
        size = img.shape

        # We send this frame to GazeTracking to analyze it
        gaze.refresh(frame)

        frame = gaze.annotated_frame()
        text = ""

        #ê³ ê°œ ë°©í–¥ ë³€ìˆ˜
        count = 0 #ê³ ê°œ ë°©í–¥ì„ ëª‡ë²ˆ ì²´í¬í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        xarr = [] #ê³ ê°œì˜ xë°©í–¥ ìœ„ì¹˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        xavg = 0 #ê³ ê°œì˜ xë°©í–¥ ìœ„ì¹˜ì˜ í‰ê· ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        yarr = [] #ê³ ê°œì˜ yë°©í–¥ ìœ„ì¹˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        yavg = 0 #ê³ ê°œì˜ yë°©í–¥ ìœ„ì¹˜ì˜ í‰ê· ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        err = 0 #ê³ ê°œ ë°©í–¥ì´ ì§€ì •í•œ ë²”ìœ„ë¥¼ ëª‡ ë²ˆ ë²—ì–´ë‚¬ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ë³€ìˆ˜

        
        if ret == True:
            faces = find_faces(img, face_model)
            for face in faces:
                marks = detect_marks(img, landmark_model, face)
                focal_length = size[1]
                center = (size[1]/2, size[0]/2)
                # mark_detector.draw_marks(img, marks, color=(0, 255, 0))
                image_points = np.array([
                                        marks[30],     # Nose tip
                                        marks[8],     # Chin
                                        marks[36],     # Left eye left corner
                                        marks[45],     # Right eye right corne
                                        marks[48],     # Left Mouth corner
                                        marks[54]      # Right mouth corner
                                    ], dtype="double")
                model_points = np.array([
                                (0.0, 0.0, 0.0),             # Nose tip
                                (0.0, -330.0, -65.0),        # Chin
                                (-225.0, 170.0, -135.0),     # Left eye left corner
                                (225.0, 170.0, -135.0),      # Right eye right corne
                                (-150.0, -150.0, -125.0),    # Left Mouth corner
                                (150.0, -150.0, -125.0)      # Right mouth corner
                            ])
                camera_matrix = np.array(
                                [[focal_length, 0, center[0]],
                                [0, focal_length, center[1]],
                                [0, 0, 1]], dtype = "double"
                            )
                dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion
                (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_UPNP)
                
                # Project a 3D point (0, 0, 1000.0) onto the image plane.
                # We use this to draw a line sticking out of the nose
                
                (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)
                
                for p in image_points:
                    cv2.circle(img, (int(p[0]), int(p[1])), 3, (0,0,255), -1)
                
                
                p1 = ( int(image_points[0][0]), int(image_points[0][1]))
                p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))
                x1, x2 = head_pose_points(img, rotation_vector, translation_vector, camera_matrix)

                cv2.line(img, p1, p2, (0, 255, 255), 2)
                cv2.line(img, tuple(x1), tuple(x2), (255, 255, 0), 2)
                # for (x, y) in marks:
                #     cv2.circle(img, (x, y), 4, (255, 255, 0), -1)
                # cv2.putText(img, str(p1), p1, font, 1, (0, 255, 255), 1)
                try:
                    m = (p2[1] - p1[1])/(p2[0] - p1[0])
                    ang1 = int(math.degrees(math.atan(m)))
                except:
                    ang1 = 90
                    
                try:
                    m = (x2[1] - x1[1])/(x2[0] - x1[0])
                    ang2 = int(math.degrees(math.atan(-1/m)))
                except:
                    ang2 = 90
                    
                    # print('div by zero error')
                #if ang1 >= 48:
                #    print('Head down')
                #    cv2.putText(img, 'Head down', (30, 30), cv2.FONT_HERSHEY_SIMPLEX , 2, (255, 255, 128), 3)
                #elif ang1 <= -48:
                #    print('Head up')
                #    cv2.putText(img, 'Head up', (30, 30), cv2.FONT_HERSHEY_SIMPLEX , 2, (255, 255, 128), 3)
                #
                #if ang2 >= 48:
                #    print('Head right')
                #    cv2.putText(img, 'Head right', (90, 30), cv2.FONT_HERSHEY_SIMPLEX , 2, (255, 255, 128), 3)
                #elif ang2 <= -48:
                #    print('Head left')
                #    cv2.putText(img, 'Head left', (90, 30), cv2.FONT_HERSHEY_SIMPLEX , 2, (255, 255, 128), 3)
                
                cv2.putText(frame, str(ang1), tuple(p1), cv2.FONT_HERSHEY_SIMPLEX , 2, (128, 255, 255), 3)
                cv2.putText(frame, str(ang2), tuple(x1), cv2.FONT_HERSHEY_SIMPLEX , 2, (255, 255, 128), 3)
            # cv2.imshow('img', img)
            
            #countê°€ 90ì´ ë  ë•Œê¹Œì§€ xarr, yarrì— ê³ ê°œ ìœ„ì¹˜ ì €ì¥
            #count 90ì€ ëŒ€ëµ 10ì´ˆ
            if(count < 90):
                xarr.append(ang2)
                yarr.append(ang1)
            #90ì´ ë˜ë©´ ê³ ê°œì˜ xë°©í–¥ ìœ„ì¹˜ì™€ yë°©í–¥ ìœ„ì¹˜ì˜ í‰ê· ì„ êµ¬í•¨
            elif(count == 90):
                print("Count = 90!!")
                for i in xarr:
                    xavg = xavg + i
                for i in yarr:
                    yavg = yavg + i
                xavg = xavg / len(xarr)
                yavg = yavg / len(yarr)
                print("xavg = ", xavg, " yavg = ", yavg)
            #countê°€ 90 ì´ìƒíˆë©´ ì‚¬ìš©ìì˜ ê³ ê°œ ìœ„ì¹˜ í™•ì¸
            #xë°©í–¥ì´ ê°€ìš´ë°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 30 -> -30ìœ¼ë¡œ í™•í™• ë°”ë€œ
            #ë§¤ countë§ˆë‹¤ ì¸¡ì • ì‹œ ì´íƒˆì„ ë„ˆë¬´ ë§ì´í•  ê°€ëŠ¥ì„± ìˆìŒ!
            elif(count % 27 == 0):
                if(ang2 > 25 or ang2 < -35):
                    print("X ë°©í–¥ Warning")
                    err = err + 1
                if(ang1 > yavg + 10 or ang1 < yavg - 10):
                    print("Y ë°©í–¥ Warning")  
                    err = err + 1         
            count = count + 1
            
            #ê¸°ì¤€ì 
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        else:
            break

        if gaze.is_blinking():
            text = "Blinking"
        elif gaze.is_right():
            text = "Looking right"
        elif gaze.is_left():
            text = "Looking left"
        elif gaze.is_center():
            text = "Looking center"

        cv2.putText(frame, text, (90, 60), cv2.FONT_HERSHEY_DUPLEX, 1.6, (147, 58, 31), 2)

        left_pupil = gaze.pupil_left_coords()
        right_pupil = gaze.pupil_right_coords()

        if frameCount == 0 and type(left_pupil) != type(None) and type(right_pupil) != type(None):
            firstLeftEye = left_pupil
            firstRightEye = right_pupil
            frameCount += 1

        if type(left_pupil) != type(None) and type(right_pupil) != type(None):
            if abs(sum(firstLeftEye)-sum(left_pupil)) >= 7 or abs(sum(firstRightEye)-sum(right_pupil) >= 7) :
                #cv2.putText(frame, "Please look straight to the screen", (90, 105), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)
                eyeErr += 1
        
        cv2.putText(frame, "Left pupil:  " + str(left_pupil), (90, 130), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)
        cv2.putText(frame, "Right pupil: " + str(right_pupil), (90, 165), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)
        
        #frame = webcam.read()[1]
        #reading the frame
        ## ì˜ìƒ í¬ê¸°ê°€ ì‘ì•„ì§„ ê±° ê°™ì–´ ì´ê²Œ ë°”ë€Œë‚˜ í•œë²ˆ ë³¸ë‹¤
        #frame = imutils.resize(frame,width=300)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)
        
        canvas = np.zeros((250, 300, 3), dtype="uint8")
        frameClone = frame.copy()
        if len(faces) > 0:
            faces = sorted(faces, reverse=True,
            key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]
            (fX, fY, fW, fH) = faces
                        # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, and then prepare
                # the ROI for classification via the CNN
            roi = gray[fY:fY + fH, fX:fX + fW]
            roi = cv2.resize(roi, (64, 64))
            roi = roi.astype("float") / 255.0
            roi = img_to_array(roi)
            roi = np.expand_dims(roi, axis=0)
            
            
            preds = emotion_classifier.predict(roi)[0]
            emotion_probability = np.max(preds)
            label = EMOTIONS[preds.argmax()]
        else: continue

    
        for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):
                    # construct the label text
                    text = "{}: {:.2f}%".format(emotion, prob * 100)

                    # draw the label + probability bar on the canvas
                    # emoji_face = feelings_faces[np.argmax(preds)]
    
                    w = int(prob * 300)
                    cv2.rectangle(canvas, (7, (i * 35) + 5),
                    (w, (i * 35) + 35), (0, 0, 255), -1)
                    cv2.putText(canvas, text, (10, (i * 35) + 23),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.45,
                    (255, 255, 255), 2)
                    #frameìœ¼ë¡œ ë°”ë€œ
                    cv2.putText(frame, label, (fX, fY - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
                    cv2.rectangle(frame, (fX, fY), (fX + fW, fY + fH),
                                (0, 0, 255), 2)
                    ##ìš”ë¶€ë¶„
        if label == "angry":
            emot[0]+=1
            
        elif label == "disgust":
            emot[1] +=1
            
        elif label == "scared":
            emot[2] +=1
            
        elif label == "happy":
            emot[3] +=1  
            
        elif label == "sad":
            emot[4] +=1
            
        elif label == "surprised":
            emot[5] +=1
            
        elif label == "neutral":
            emot[6] +=1    
            
        #cv2.imshow("Demo", frame)
        # cv2.imshow('your_face', frameClone)
        # cv2.imshow("Probabilities", canvas)

        if cv2.waitKey(1) == 27:
            break
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
   
        # ret, buffer = cv2.imencode('.jpg', frame)
        # frame = buffer.tobytes()
        # yield (b'--frame\r\n'
        #        b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')  # concat frame one by one and show result    

        endTime = Time.time()
        good = (emot[3]+emot[6])/(emot[0]+emot[1]+emot[2]+emot[3]+emot[4]+emot[5]+emot[6])
        gp=100*good
        igp=math.trunc(gp) # ë©´ì ‘ê°„ì˜ í‘œì •ì´ ì–¼ë§ˆë‚˜ ì¢‹ì•˜ëŠ”ì§€ í¼ì„¼í…Œì´ì§€

        interviewTime = endTime-startTime
        
        global fault
        fault = [float(err/interviewTime), igp, float(eyeErr/interviewTime)]
        print(f'fault: {fault}')

    webcam.release()

def makeRequest(messages):
    return openai.ChatCompletion.create(
                model = "gpt-3.5-turbo",
                messages = [messages]
            )

def makeQuestion(article):
    text = ""
    pd_text = []
    pd_text_row = []

    article = article.split(".")

    for text in range(len(article)):
        pd_text_row.append(article[text])

        if (text + 1) % 20 == 0:
            pd_text.append("".join(pd_text_row))
            pd_text_row = []

    result = ""

    print("í…ìŠ¤íŠ¸ë¥¼ gpt ë…€ì„ì—ê²Œ ìš”ì•½ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.")
    print("ê¸´ í…ìŠ¤íŠ¸ëŠ” 15ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ gptê°€ ê¸°ì–µí•©ë‹ˆë‹¤.")

    #ìê¸°ì†Œê°œì„œ ë‚´ìš© ìš”ì•½. resultì— ìš”ì•½í•œ ë‚´ìš© ì €ì¥
    for i in range(len(pd_text)):
        currentText = pd_text[i]

        question = {"role":"user", "content": "ë‹¤ìŒ ë‚´ìš©ì„ ì½ê³  í•œêµ­ë§ë¡œ ìš”ì•½í•´ì¤˜.\n" + currentText}
        completion = makeRequest(question)
        response = completion['choices'][0]['message']['content'].strip()
        result += response

        print(f"{i + 1}ë²ˆì§¸ í…ìŠ¤íŠ¸ë¥¼ gptë…€ì„ì´ ê¸°ì–µí–ˆìŠµë‹ˆë‹¤.")
    
    question = {"role":"user", "content": "ë‹¤ìŒ ê¸€ì„ ì½ê³  í˜„ì¬ ë©´ì ‘ ì¤‘ì´ê³  ë„ˆê°€ ë©´ì ‘ê´€ì´ë¼ ìƒê°í•˜ê³  í•œêµ­ë§ë¡œ ì§ˆë¬¸ì„ í•œì¤„ì”© ë„ì›Œì„œ ì„¸ ê°€ì§€ í•´ì¤˜. ì„¸ ê°€ì§€ ì§ˆë¬¸ì˜ ìœ í˜•ì€ ë‹¤ìŒê³¼ ê°™ì•„. ì²«ë²ˆì§¸ ì§ˆë¬¸ì€ ìê¸°ì†Œê°œì„œì™€ ê´€ë ¨ëœ ì§ˆë¬¸, ë‘ë²ˆì§¸ëŠ” ì§€ì›ìê°€ ì§€ì›í•œ ì§ë¬´ì— ì¶©ë¶„í•œ ì§€ì‹ì´ ìˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸, ì„¸ë²ˆì§¸ëŠ” ì§€ì›ìì—ê²Œ ê¹Œë‹¤ë¡œìš´, í•œë²ˆ ë” ìƒê°í•´ì•¼ í•˜ëŠ” ì§ˆë¬¸ì´ì•¼.\n" + result}

    print("gptê°€ ì§ˆë¬¸ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤.")

    completion = makeRequest(question)

    response = completion['choices'][0]['message']['content'].strip()

    return response.split('\n')

@app.route('/')
def home():
    return render_template('index1.html')

@app.route('/video')
def index():
    return render_template('liveCam2.html')

# @app.route('/video', methods=['POST'])
# def startVideo():
#     return start()

# ì´ startì—ì„œ ë°˜í™˜ëœ ê°’ì„ ì•„ë˜ì˜ interviewResult í˜ì´ì§€ì—ë‹¤ ë„˜ê²¨ì£¼ì–´ì•¼ í•˜ëŠ”ë° ì´ê±¸ ì–´ì¼€ í•´?
@app.route('/video_feed') 
def video_feed():
    start()
    return jsonify('success')
    #return Response(start(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/interviewResult')
def returnList():
    return render_template('interviewResult.html')
    
@app.route('/pdf', methods=['POST'])
def getPdfText():
    # íŒŒì¼ ì½ê¸° ì‹œì‘
    print("íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤")
    
    # jsë¡œ ìš”ì²­ ë„£ì€ íŒŒì¼ì„ ê°€ì ¸ì˜¨ë‹¤.
    file = request.files['file']

    # PyPDF2 ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ PdfReader ë©”ì„œë“œë¥¼ í†µí•´ íŒŒì¼ì„ ì½ëŠ”ë‹¤.
    reader = PyPDF2.PdfReader(file)
    
    article = ''
    
    # í˜ì´ì§€ ë³„ë¡œ ì½ì–´ì˜¨ pdf íŒŒì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ article ë³€ìˆ˜ì— ë”í•´ì¤€ë‹¤.
    for page_num in range(len(reader.pages)):
        page = reader.pages[page_num]
        article += page.extract_text()

    # ë§Œë“  ì „ì²´ ìì†Œì„œ í…ìŠ¤íŠ¸ë¥¼ gptì— ë„˜ê²¨ ì§ˆë¬¸ì„ ìƒì„±í•œë‹¤.
    question = makeQuestion(article)

    # ìƒì„±ëœ ì§ˆë¬¸(string type array)ì„ questionArrayë¼ëŠ” keyê°’ì˜ valueë¡œ ì„¤ì •í•œ dictionaryë¥¼ ë§Œë“ ë‹¤.
    # ë§Œë“¤ì–´ì§„ dictionaryë¥¼ jsonify ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ jsoní˜•íƒœë¡œ ë³€í™˜í•œ í›„ ë°˜í™˜í•´ì¤€ë‹¤.
    return jsonify({"questionArray" : question})

@app.route('/stop-video', methods=['PATCH'])
def stopVideo():
    global isEscape
    isEscape = True
    print(f'isEscapeê°€ {isEscape}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤!')
    return jsonify('success')

@app.route('/answerFeedback', methods=['POST'])
def getFeedback():
    data = request.json()
    feedback = data["feedbackList"]
    

@app.route('/fault', methods = ['GET'])
def getFault():
    global fault
    faultMessage = ["ğŸ“ ", "ğŸ˜ ", "ğŸ‘ï¸ "]
    count = 0
    if(fault[0] >= 0.13): faultMessage[0] += "ê³ ê°œ ë°©í–¥ì„ ìì£¼ ë³€ê²½í•©ë‹ˆë‹¤. ë©´ì ‘ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¼ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤." 
    elif(fault[0] >= 0.067):  faultMessage[0] += "ê³ ê°œ ë°©í–¥ì„ ì£¼ì˜í•´ì£¼ì„¸ìš”. ë” ì•ˆì •ì ì¸ ìì„¸ë¡œ ê³ ê°œë¥¼ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤."
    else: 
        faultMessage[0] += "ê³ ê°œ ë°©í–¥ì´ ì•ˆì •ì ì…ë‹ˆë‹¤. ë©´ì ‘ì— ì§‘ì¤‘í•˜ëŠ” ì¸ìƒì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        count += 1

    if(fault[1] <= 60): faultMessage[1] += "ë©´ì ‘ìƒí™©ì— ì ì ˆì¹˜ ì•Šì€ í‘œì •ì…ë‹ˆë‹¤."
    elif(fault[1] <=85): faultMessage[1] += "ì¡°ê¸ˆë” ìì‹ ê° ìˆëŠ” í‘œì •ìœ¼ë¡œ ì°¸ê°€í•´ì£¼ì„¸ìš”."
    else: 
        faultMessage[1] += "ë©´ì ‘ìƒí™©ì—ì„œ ì¢‹ì€ í‘œì •ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
        count += 1

    if(fault[2] <= 1.00): 
        faultMessage[2] += "ì‹œì„ ì´ ê±°ì˜ í”ë“¤ë¦¬ì§€ ì•Šê³  ì•ˆì •ì ìœ¼ë¡œ, ì‹ ë¢°ê°ì„ ì¤„ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
        count += 1
    elif(fault[2] <= 3.00): faultMessage[2] += "ì‹œì„ ì´ ì¡°ê¸ˆ í”ë“¤ë¦¬ì§€ë§Œ í‰ê· ì ì…ë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ë” ì‹œì„ ì„ ê³ ì •í•´ì£¼ì„¸ìš”!"
    else: faultMessage[2] += "ì‹œì„ ì´ ìì£¼ í”ë“¤ë¦½ë‹ˆë‹¤. ë©´ì ‘ ìƒí™©ì—ì„œëŠ” ì‹œì„ ì„ ë˜‘ë°”ë¡œ ìœ ì§€í•´ ì£¼ì„¸ìš”."

    if(count == 3): faultMessage.append("í•©ê²©")
    else: faultMessage.append("ë¶ˆí•©ê²©")
    
    return jsonify({"faultArray" : faultMessage})

if __name__ == '__main__':
    #app.run('127.0.0.1', 5000, debug=True)
    app.run(debug=True)